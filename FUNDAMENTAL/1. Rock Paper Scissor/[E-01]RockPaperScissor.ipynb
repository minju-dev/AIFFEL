{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31978ece",
   "metadata": {},
   "source": [
    "### 모델학습 : 내가 찍은 사진 / 모델테스트 : 팀원 중 1명의 사진"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2e8395ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images to be resized.\n",
      "100  images to be resized.\n"
     ]
    }
   ],
   "source": [
    "# (1) 필요한 모듈 import하기\n",
    "from PIL import Image \n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# (2) 데이터 준비 + Resize하기\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "\n",
    "# 가위바위보 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fabaf02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 300 입니다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300):  # 가위바위보 이미지 개수 총합에 주의\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역 생성\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4b11731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "87809f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 26, 26, 65)        1820      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 13, 13, 65)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 11, 11, 40)        23440     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 5, 5, 40)          0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 125,663\n",
      "Trainable params: 125,663\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(65, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(40, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "36a1b65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0992 - accuracy: 0.3800\n",
      "Epoch 2/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0696 - accuracy: 0.4267\n",
      "Epoch 3/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0062 - accuracy: 0.5800\n",
      "Epoch 4/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8831 - accuracy: 0.6767\n",
      "Epoch 5/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7333 - accuracy: 0.7333\n",
      "Epoch 6/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6549 - accuracy: 0.7433\n",
      "Epoch 7/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5596 - accuracy: 0.7833\n",
      "Epoch 8/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5487 - accuracy: 0.7700\n",
      "Epoch 9/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4696 - accuracy: 0.8267\n",
      "Epoch 10/25\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4164 - accuracy: 0.8700\n",
      "Epoch 11/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3992 - accuracy: 0.8633\n",
      "Epoch 12/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3555 - accuracy: 0.8767\n",
      "Epoch 13/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3161 - accuracy: 0.8967\n",
      "Epoch 14/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2874 - accuracy: 0.9200\n",
      "Epoch 15/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2848 - accuracy: 0.9067\n",
      "Epoch 16/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2333 - accuracy: 0.9233\n",
      "Epoch 17/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.9400\n",
      "Epoch 18/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1875 - accuracy: 0.9533\n",
      "Epoch 19/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.9600\n",
      "Epoch 20/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1393 - accuracy: 0.9767\n",
      "Epoch 21/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1610 - accuracy: 0.9500\n",
      "Epoch 22/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1218 - accuracy: 0.9733\n",
      "Epoch 23/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1182 - accuracy: 0.9600\n",
      "Epoch 24/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0971 - accuracy: 0.9767\n",
      "Epoch 25/25\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f53782f67c0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ac3989bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "학습데이터(x_train)의 이미지 개수는 300 입니다.\n",
      "x_train shape: (300, 28, 28, 3)\n",
      "y_train shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/test/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/test/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/test/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "x_test_reshaped = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_test_reshaped.shape))\n",
    "print(\"y_train shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "66498b8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 0.8908 - accuracy: 0.6500\n",
      "test_loss: 0.8907809257507324 \n",
      "test_accuracy: 0.6499999761581421\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a564e4",
   "metadata": {},
   "source": [
    "### 회고\n",
    "\n",
    "#### 1. 이번 프로젝트를 진행하면서 어려웠던 점\n",
    "딥러닝과 관련된 클래스, 메소드 등을 직접 사용하면서 코드를 짜본 것은 이번이 처음이였기에, 설명을 읽고 진행하더라고 각 클래스와 메소드, 함수 등이 어떤 역할을 하고 기능을 하게 되는지에 대한 배경 지식이 없어 이해하기가 힘들었다.\n",
    "\n",
    "#### 2. 프로젝트를 진행하면서 알아낸 점 혹은 아직 모호한 점\n",
    "1) 알아낸 점\n",
    "  \n",
    "  A. 모든 모델을 구현하는데 있어서 데이터 전처리는 매우매우 중요하다.\n",
    "  모델에 데이터를 넣어주기 전 데이터가 제대로 처리되어 있지 않으면 코드가 실행조차 되지 않았고, 실제 데이터를 만들고 코드를 짜보면서   코드 자체를 써내려 가는 시간보다 데이터 전처리 시간이 더 많이 소요된다는 것을 알았다. \n",
    "  \n",
    "  B. 데이터의 질\n",
    "  데이터의 질 자체가 너무 좋지 않은 경우에는 모델 자체가 굉장히 좋은 모델이라 하더라도 결과가 엉망일 가능성이 높다. \n",
    "  \n",
    "  C. 과대적합(Overfitting)과 과소적합(Underfitting)\n",
    "  훈련세트와 테스트 세트의 정확도(점수)를 비교했을 때 훈련세트가 너무 높으면 과대적합, 그 반대이거나 두 점수 모두 낮으면 과소적합이라\n",
    "  고 한다. 발생하는 원인은 여러가지이다. 모델이 너무 단순하거나, 규제가 너무 많거나, 그냥 단순히 충분히 오래 훈련하지 않는 경우이다.\n",
    "  즉, 네트워크가 훈련 세트에서 적절한 패턴을 학습하지 못했다는 뜻이다. 항상 기억해야 할 점은 딥러닝 모델이 훈련 세트에는 학습이 잘 되\n",
    "  는 경향이 있지만 진짜 해결할 문제는 학습이 아니라 **일반화**라는 것이다.\n",
    "  \n",
    "  D. 에포크와 과대적합, 과소적합의 관계성\n",
    "  모델에 따라서 에포크 회수에 따라 과소적합이나 과대적합이 일어날 수 있다. 에포크 횟수가 적으면 모델이 훈련 세트를 덜 학습하게 되고, \n",
    "  에포크 횟수가 크면 훈련 세트를 완전히 학습해 훈련 세트에만 너무 잘 맞는 모델이 될 가능성이 높아진다.\n",
    "  \n",
    "  E.재밌다.\n",
    "  코드를 직접 짜 보면서 머리가 아프기도 하지만 그만큼 재밌다는 생각을 많이 하게 된다.점점 재미를 붙여가고 있고 이 흥미가 아이펠이 끝\n",
    "  날 때까지 유지되길 바란다. :-) \n",
    "  \n",
    "  → 알게된 점(과대, 과소적합, 에포크와의 관계성)에 대해서는 추후 벨로그 정리 예정\n",
    "\n",
    "2) 모호한 점\n",
    "  \n",
    "  A. 에포크와 과대적합, 과소적합의 관계성\n",
    "  모든 모델에서 위에서 본 관계성이 나타나는 것인지?\n",
    "  \n",
    "  B. 모델 구현 시 사용된 Layer들(Conv2D, MaxPool2D, MaxPooling2D, Flatten, Dense)의 역할과 기능\n",
    "  \n",
    "  C. 과대적합을 보완하기 위한 방법으로 '가중치 규제', '드롭아웃 추가'를 텐서플로우 사이트에서 소개하고 있었으나, 이해하기가 어려워 적\n",
    "  용해보지는 못 했다. \n",
    "  \n",
    "  D. 'images=glob.glob(img_path + \"/*.jpg\")', 'image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/rock\"'\n",
    "  위 코드에 작성된 부분을 확인하다 보면 앞과 같이 쓰여진 부분이 있는데, 이 부분이 아직까지 완전히 이해된 상황이 아니다. 좀 더 공부가 \n",
    "  필요할 듯하다.\n",
    "  \n",
    "  E. 크로스엔트로피 손실함수(cross-entropy loss function) 사용해보기\n",
    "  다중 분류에서 사용하는 손실 함수. 손실 함수를 머신러닝 알고리즘이 얼나마 엉터리인지를 측정하는 기준이다. 지금 작성된 알고리즘이 꽤 \n",
    "  엉터리라 생각되서 한 번 사용해보고 싶었는데, 지금 단계에서는 너무 어렵게 느껴져 시도해보지 못 했다. 다음에 기회가 된다면 사용해보고 \n",
    "  싶다.\n",
    "  \n",
    "  → 모호한 점도 같이 공부해서 벨로그에 업로드 하기\n",
    "\n",
    "#### 3. 루브릭 평가 지표를 맞추기 위해 시도한 것들\n",
    "#### 4. 루브릭 평가 지표를 달성하지 못했을 때, 이유에 관한 추정\n",
    "하단에 루브릭 평가 지표를 맞추기 위해 시도한 것들과 루브릭 평가 지표를 달성하지 못했을 때, 그 이유에 대한 추정을 작성해 보았다.\n",
    "\n",
    "##### 처음에 정확도가 너무 낮게 다와 다양한 방법을 사용해 정확도를 높여 보았다.\n",
    "\n",
    "**1)** Conv2D 레이어에서 입력 이미지의 특징 수를 늘리거나 줄여 보거나, Dense 레이어에서 뉴런수를 바꾸어 보거나, 학습 반복 횟수인 epoch 값을 변경 (Conv2D(입력 이미지 특징 수), Densd(뉴런수), epoch(학습 반복 횟수) 값을 늘리기)\n",
    "(아래 변경 전 결과값은 '[E-01] RockPaperScissor_test' 파일에서 확인 가능)\n",
    "* 변경 전 : 10, 20, 32, 3, epoch = 10\n",
    "  * test_loss: 0.9922240972518921 \n",
    "  * test_accuracy: 0.43666666746139526\n",
    "\n",
    "* 변경 후 : 65, 40, 100, 3, epoch = 25   \n",
    "  * test_loss: 0.8907809257507324 \n",
    "  * test_accuracy: 0.6499999761581421\n",
    "  * accuracy가 현 시점에서는 60%를 넘기고 있으나 코드를 실행할 때마다 값이 다르게 나오며 60% 아래로 떨어지는 경우도 있었음.\n",
    "    (모델 자체가 많이 불안정하다 판단된다.)\n",
    "\n",
    "→ accuracy가 약 65%정도까지 올라가긴 했으나 여전히 loss로 그렇게 좋은 모델이라고 볼 수 없다. \n",
    "   test 데이터셋으로 학습 시 accuracy가 약 98%까지 나오는 것을 감안하면 데이터가 과적합(Overfitting) 되었다고 판단된다.\n",
    "   데이터가 과적합(Overfitting)된 원인에 대해서는 아래와 같이 추정해 볼 수 있을 것이다.\n",
    "   \n",
    "   A. epoch값이 너무 크다 → **2)** \"다른 하이퍼파라미터 값은 변경된 값으로 그대로 두고 epoch만 10으로 변경\" (결과는 하단 부록 참고)\n",
    "   \n",
    "   B. 데이터 자체의 문제 \n",
    "    \n",
    "    B-1. 학습에 사용한 이미지(내가 만든 이미지)의 경우 가위, 바위, 보 사진을 한 방향이 아닌 여러 방향에서 찍힌 이미지를 사용하였으        나 테스트에 사용한 이미지(팀원 이미지)의 경우 한 방향에서만 찍힌 사진을 사용하였다. 여러 방향에서 찍힌 이미지를 사용해 학습을 하      다보니 다양한 정보를 충분히 학습하지 못 했을 것이라 판단된다. \n",
    "     → **3)** \"학습 데이터량을 늘려준다.\" (하이퍼파라미터는 변경 후 값 사용)\n",
    "     \n",
    "     B-2. 이미지의 해상도 및 크기 또한 결과값에 영향을 미쳤을 것이라 생각된다. 상단에 출력된 이미지나 \n",
    "     '[E-01] RockPaperScissor_test'파일에서 출력된 이미지를 보면 사람도 자세히 보아야지만 가위, 바위, 보 구분이 가능할 정도이다. \n",
    "     → \"크기를 키우고 해상도를 높인다.\" 머신러닝에서 많이 사용하는 이미지 크기는 32X32 ,64X64 ,96X96 ,245X256으로 여기서는 96X96 \n",
    "     사용 (하이퍼파라미터는 변경 후 값 사용)\n",
    "     \n",
    "     \n",
    "**2)** 사이킷런의 RandomizedSearchCV클래스를 사용한 하이퍼파라미터 튜닝\n",
    "RandomizedSearchCV클래스의 경우 하이퍼파라미터 탐색과 교차 검증을 동시에 수행하므로 B-1에서 만들었던 900개의 샘플로 진행한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d8f4c6",
   "metadata": {},
   "source": [
    "### 부록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7f4a6",
   "metadata": {},
   "source": [
    "#### A. epoch값이 너무 크다  →  다른 하이퍼파라미터 값은 변경된 값으로 그대로 두고 epoch만 10으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bc90edf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1396 - accuracy: 0.9600\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.9867\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.9867\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9933\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0426 - accuracy: 0.9933\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0372 - accuracy: 0.9967\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 0.9967\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.9967\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f537822bf40>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "53da3262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "학습데이터(x_train)의 이미지 개수는 300 입니다.\n",
      "x_train shape: (300, 28, 28, 3)\n",
      "y_train shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/test/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/test/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/test/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "x_test_reshaped = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_test_reshaped.shape))\n",
    "print(\"y_train shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "df1301fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 1.0879 - accuracy: 0.6300\n",
      "test_loss: 1.087900996208191 \n",
      "test_accuracy: 0.6299999952316284\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e4b8b3",
   "metadata": {},
   "source": [
    "epoch를 낮추어도 정확도가 62%까지 나오는 것으로 보아 epoch값으로 해결 할 수 있는 문제라고 판단되지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3acca",
   "metadata": {},
   "source": [
    "#### B-1. 다양한 정보를 충분히 학습하지 못 했을 것 \n",
    "#### →  학습 데이터량을 늘려준다. (학습 데이터 : 내 이미지, 다른 팀원 2명의 이미지 추가, 테스트 데이터 : 이전과 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0343a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300  images to be resized.\n",
      "300  images to be resized.\n",
      "300  images to be resized.\n"
     ]
    }
   ],
   "source": [
    "# (1) 필요한 모듈 import하기\n",
    "from PIL import Image \n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# (2) 데이터 준비 + Resize하기\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "\n",
    "# 가위바위보 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor_1\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock_1\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper_1\"\n",
    "resize_images(image_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c80b9a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 900 입니다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=900):  # 가위바위보 이미지 개수 총합에 주의\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역 생성\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor_1/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock_1/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper_1/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "65c24bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_34 (Conv2D)           (None, 26, 26, 65)        1820      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 13, 13, 65)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 11, 11, 40)        23440     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 5, 5, 40)          0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 125,663\n",
      "Trainable params: 125,663\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(65, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(40, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e8264502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "29/29 [==============================] - 1s 4ms/step - loss: 0.1064 - accuracy: 0.9756\n",
      "Epoch 2/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0677 - accuracy: 0.9811\n",
      "Epoch 3/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0451 - accuracy: 0.9922\n",
      "Epoch 4/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0421 - accuracy: 0.9900\n",
      "Epoch 5/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0420 - accuracy: 0.9878\n",
      "Epoch 6/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.9933\n",
      "Epoch 7/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0149 - accuracy: 0.9978\n",
      "Epoch 8/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0155 - accuracy: 0.9944\n",
      "Epoch 9/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.9956\n",
      "Epoch 10/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0100 - accuracy: 0.9978\n",
      "Epoch 11/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9956\n",
      "Epoch 12/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 0.9989\n",
      "Epoch 13/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 14/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 15/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 16/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 17/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 18/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 19/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 20/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 9.3171e-04 - accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 8.9392e-04 - accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 7.6478e-04 - accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 7.1138e-04 - accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 7.0958e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f535c2d7f70>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "563110d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "학습데이터(x_train)의 이미지 개수는 300 입니다.\n",
      "x_train shape: (300, 28, 28, 3)\n",
      "y_train shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/test/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/test/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/test/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "x_test_reshaped = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_test_reshaped.shape))\n",
    "print(\"y_train shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "04edb43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 2.7716 - accuracy: 0.6367\n",
      "test_loss: 2.7716474533081055 \n",
      "test_accuracy: 0.6366666555404663\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d578df7d",
   "metadata": {},
   "source": [
    "학습데이터량을 클래스 별로 200개씩 늘렸음에도 불구하고 accuracy에는 유의미한 차이를 보이지 않았다. 여전히 loss 값은 높게 나오므로 모델 자체가 불안정하다 판단되며, accuracy에 영향을 줄만큼 데이터량이 충분히 제공되지 못 한것으로 해석된다. 학습 데이터량으로 accuracy를 높이기 위해서는 더 많은 양의 데이터가 필요한 것으로 판단된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aac722",
   "metadata": {},
   "source": [
    "#### B-2. 이미지의 크기와 해상도가 낮은 것 →  크기를 키우고 해상도를 높인다.\n",
    "#### 해상도를 높이는 방법 하나 자체만으로 모델을 따로 구현해야 할 정도로 복잡하다. 해상도 부분은 제외하고 크기만 키워 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "306e00e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images to be resized.\n",
      "100  images to be resized.\n"
     ]
    }
   ],
   "source": [
    "# (1) 필요한 모듈 import하기\n",
    "from PIL import Image \n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# (2) 데이터 준비 + Resize하기\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 96x96 사이즈로 바꾸어 저장\n",
    "\ttarget_size=(96,96)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "\n",
    "# 가위바위보 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8208d266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 300 입니다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300):  # 가위바위보 이미지 개수 총합에 주의\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=96\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역 생성\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = (x_train)-10/206.0   # 입력은 0~1 사이의 값으로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d2aa7e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABkD0lEQVR4nO29X6x9z1Uf9lmzz/05JbED2NRybVpMgxJZkSiRRYmoqggSldIo5AEhkihyUyq/pAlJIyXQPqStWilRUwgPVSoLGqEKFQhBAdEoaerAQ15cIKAm4Di4kIAt8ycCmhaR7z17ZvVhrTWzZvbMPvuce+69536/e12du8/Ze/bM7NnzWf9mzQwxM3baaafXn8JzV2CnnXZ6GtrBvtNObwjtYN9ppzeEdrDvtNMbQjvYd9rpDaEd7Dvt9IbQg8BORF9FRJ8gok8S0Tddq1I77bTT9YkuHWcnognAPwXwBwB8CsCPAvgjzPzT16veTjvtdC06PODeLwXwSWb+WQAgou8G8DUAhmB/+zvezp/3r3/eA4oEOP97ONF1snlRtOTtFzbmlfI5667VxMuLm/J+rOfoZtOepJVrl9XoV3/1V/Eb/99vdLv2Q8D+XgC/4H5/CsC/2yYiog8D+DAAvPPz3oX/5r//7x5QJMDMuErUHwMTCOER3Ra9ej5FxOJaGe211fowA6l//ax8VuicfNba89xrozQPfY5ePqO8iWiRZi3vLWm/5X/41uG1R3fQMfNHmPmDzPzBd7zj7Y9d3E477TSgh4D90wA+3/1+n57baaedbpAeAvYfBfBFRPR+InoLwNcD+MHrVGunnXa6Nl1sszPzTET/GYC/C2AC8D8z809drWY77bTTVekhDjow898G8LevVJeddtrpEWmPoNtppzeEdrDvtNMbQi8O7IQ3Mxhmp50eSg+y2S+hhwLVwgl2wO+003n04iT7TjvtdBntYN9ppzeEdrDvtNMbQjvYd9rpDaEd7Dvt9IbQDvaddnpD6MmH3jqrJ1yYz8q1LeNydCKPrdXY8DzPMa/90nnhLW0d4iSis8tcu/6Sdio6Z768fe+111Xe2UqzPS3YGcPFEM6htQ64vDZOTU8wWv+SgQ7GkCn2OuspwG+t00sCuqfRAhajhSwuXbziVPoRPb1kfyBtE9odyFNz7gn603OtVHMOna4PwZpuy0ori3Y+UcbrDvRemmuVdS7tNvtOO70htIN9p53eEHpxavxOT08PcbzdIl2zzi/p+XfJvtNObwi93KG3nquO9CyfOLfTRrpMor8kaTeq69oz9JyQL4GeAexXyqeHdV6e753baTsxn9d8LwnoLb3kum+hF2uzDzsgAwvG2zn3mr/XR6FLpflLANE5m0SsDTPeMu02+047XUgvgYl5emLJ/vCtm14aN30d6HWW6Gs0DCF+oX3wxUn2l96BXhy9xu39OjOqHr04sO+00zXodQTzKXqxDrqdnpcuGbK6RepNUnldaZfsO+2E1x/owC7Zd7oSvS5gOXd9gpfkrNsl+047vSH09ItXnCUB+iGxOa8t1wbhsqsL1XQvjFdy4cHltWujMq44RWPY1Neca53v0oe9tP7b8ulf8KcW8+P1qrwHd40GXXHjcxAAEPncBzVepihrgnTewzCfbefW6GZi4xcLTgy0I7LwzSuEyxLorAYb4WFVkTvjLZ3qONvpekAfXumBZQSgNTozn14blei3tWsDbgwS0OVVdmgTy2Vd1MOy5ep/e+z9Wi6xsrmtL2SsT7/904o4rewf9+Jau2h1v7et4bI5r87KKj2Oe0aMeGXTdeo3vE9Un42lbCu/V8rWtJwZZW/1mfo0lwY9i87Jh3n5brasDLPaHgZ4doDfaIez3u/r1R5H54p89/UYlNNro1H6lXxehM3+ujh/dtrpOelFgH2nnXZ6ON3OFFeirMJVmkhWs6qfq/nUDjpbLXGU/kR9z6DXJdBkp9eTnmGcfYAudteoPa8ns7c9jEHK1NxPVwX0Tju9VHpysHedoo1DJEvxnlctnztj9fjqvuvSo8wI25nTTo9AL85m31XinXa6jE6CnYg+n4h+mIh+moh+ioi+Uc9/LhH9PSL6GT1+zqWVYO7Pc7fzLw3ga3V+ic+z0+tBWyT7DODPMfMHAHwZgD9JRB8A8E0APsrMXwTgo/p7p512ulE6CXZm/gwz/0P9/v8C+DiA9wL4GgDfqcm+E8AffqQ67rTTTlegsxx0RPQFAL4EwMcAvJuZP6OXfhHAuwf3fBjAhwHgne9856Y9wc7dL+wW6BqOOh6HF+6004Nps4OOiH4bgL8J4M8w87/011h6bLenMvNHmPmDzPzBt7/97Q+qrMvzKvncHL2mj7XTbdAmsBPRHQTo38XM36+nf4mI3qPX3wPgl88pmLl8mtIG5+s051173LGsS5xxu6Nup6emLd54AvAdAD7OzN/iLv0ggA/p9w8B+IFtRfppLNT5tOnaT1i5dm76nXZ6c2iLzf7lAP44gH9ERD+p5/4LAH8JwPcS0TcA+OcAvm5bkYMZVOjZ6udK8LVrmycUPirt0nyn56KTYGfmf4Axsr7yutXZaaedHouebQ263jpez7Gtzrle9N45ygsf9NNtLqON8h2UtaVOZ5U7TNuGMZ9uj9H6bKc0mmuunnNLdM216be29YhuYsFJZt4McCJ6EmZwTufzdfKOt7XFE4bP8MBHO3fBRF/HxSIhzYzDNo8RQzv3/dwS0HuMe416UZ6nFs04pz9cc6nrmwA7cBrwdu2aQB813bmdz4M9pZSPPY/7Neo/kpgjD38vfa9D9t+BLb+0LGf0fOcw73Pa+qkYwDmAPxfoa/msvddr0M2A/SWTvageAHzHeQyG5evQo6W07oOxPbdF0vj8/D0P0b4e2rGvBYxzTJBboC113MF+JTol7XrfH5tG5XqTo/WXtGlTWjdFfB6Pycx2ejg9G9jHHWJ5vqTd0om2BtWsLXfTSd0BhdFIdRs9Ywu2h9isHnAjwG4B+vh9lJVqt9iao7KuSeeo2JemeWidr6UZbFXnS1uP83pisG8JZmmB3btnLRbonLH6h9MpoK9J9daZx8yZ/5zj8FqzMbdoFe39pU7jznOO9D7X6fWm06V2/ym6nWWpumnWOurjBdWsSaTeMOEpid4D/uj+9kWPbOxL6nGOpKvrtEx3K6r6Q6T3uWm35LNFmxmlGUnxtXTLvMbl3rzN/pidijZuCHBR3o2zyj7+RZnH3p83b/6pPH3edk8IASmlk202MhuWnnrg1sKK33QNoQf8rRh5UrCfqlOrcvYe4trgX7OttlzbMiQXQqh+j/Jrh+vWTIH2nL10K8uXscUnMAb7eptvaYenAOia9Lvk/nPy2mqf97Sx9r6t0v6c+hndvGR/qeSBPpLsHrhZsscERpH2HrztuVNq+shE2EIl7cOY63NI4luT/lt9FpcA/Ry6SbDfij14KbWq+0iV74ExpljlMbLdR9K+NQMWDsBOXR+LHsvRtKXMW6NznZTXBjrwHHu9XdC5ngr85zjmRue3mB5Dle2E/+BcW9wAzszZRPB5eQZyaRuf6pSPAb5Lh96ew1H30HKvyTBvUrI/GdF5Y+3bs6UhgJZ2sQvE4X66U8NnI+ntQZ5SqqR+a2IAyOZBbb/3vfFPBew3hXqS/9qa0e1s/wQg7+3EcKaijLNTm7CXD3Wu9c5ZjtSXpVskvNEpCeClZxfgWILUl7NlnNzXoz2ugd0/Z688ubZ0rJ7j+Os90zXpElv40rTnSPRz22GLFvLQtn0GyR6wglT3y4N8YwQdo9MzO+eqa+tZXkIevDYU5l9GC0D79JxvIy2hLaNV2WOM4gNwR18X7+xr869/9yPoet57+/4UoyivG43a85r0DBF0/thepeaX/naA7e2nXtFmwI+HP3INzrC7elJ8LXii92nL7Dn32vzaPDwjWQO7jcn7PEbSvWXOPTt9raM+BdAf04S4tkTfoj2c+zxb0r/ZNvuVqSfhWkdYD5DZeUaohtqAZbxBy4B6II8x4v7+vpLwpj20Q4LMjGmaFmXJNYBI1J81X8Oak65lfDtdRi/SQbdlCOKldYwt9pY/3wOpZw5GPane5ueB7ME+z3P+7q9bXtM0VYyg917I/B0DR0sP7P45d6Bfh16ugw6D4alysfq9SGkPPuhAvbOjrsZd/0E2UJe55HNuCAsEQgDQCXNlBQQ431MBJCUEZoETEUAE6kh2u8+OZABkBuW8GZwSUpzBKQIpASkizkekGDHPETFFBAqYpgCeJhwCgDCBD0ECeQIQEGRNXis3MBIDSPocnJA4Qqqf6rrlOkvLCNABZhqYX8Xjvzjb5Jmv8dKpunwrJ66dMslWrw7u6Zk2nbJOqulExSt6LshPMNWnl+y9c20l6aRlvj3v1dS0eLEEP9bkrrL/zdU1HbhqckpAT4IDOf+geQR9Xg4BrOp1LrZxxIGt5vaPQSx1TikizjNSjECSz/zqFWKccX9/j+PxiGmacJgmHA4TDpQwTQekA2FCAk1A4IAQAg5B2iZpe0SO8jyIYI5IifNc96qJcl81JmWtY0zR0rZgPh062ryR6vdmoK+cu4R6zKj3vfo9Ylgt0M8B/i1649dUuvbaatq1Mjac63UykZajuxhepTV21HOsbfFal3z0Q2Rfqvy8rb/2fODCpBgAs6nutcefiJCIkJJE24WQwFVMPg87Vu91VM8kKooC3tR3azcsm3OUz4CuluZkiobWgHTCZOvV6azyPeAfSLuDbkFX4vknHFeeslMMtHipQx/Hav9jzFGk/Nx82DGFGCMsxLYdvvPlGzNaeuw7+neD751uh24C7M/pwGklpu+n1xj+GEl2e+TK+23mS2One8BTIyS7NWTO0rodx+eUkIjAXIN75GzzVHvqB9K/8823xSm6Vppz0l2Drlnvx6KbAPutEKuz7AoZZe1u7LHGAg/ZUYfiie/NeR+RmvAi2eeIeT5inmccj8f8maYJB83zrbeOYJYxeBt3X0h2V5fNoyiVzyV7GHY6QY/NDJ4P7N7GtVPNNf/o62EedZrefec042qjG4hXcjTL+aTn1ZFIdlSSPV/TPInIPciKZHUS3YbeYoyIMeY0IdBS6vck+5le4WrocAXkpZwtacb0kDSPAa6H1nltGHeL1rVGr5Vk75mKa+bj5a/6Op0kBwM7m50XQLdrrfNwpXZcHHQZ8AZ21V6ICPO8wWZ33n8bYPTnMJD4pfNpnMCw7ucZ+NcC6DWBfo28RibgNem1AjuwHfBXG3p5yAuhhiOT9/fnU3LMGsWmWi3i7g3wJqWJgBhnhECLwJte2aR+vQx0Rpb6bZhtfTwxffaM5ntdgf5U+T492Nde/KXXrkkbVb5zatONTmuuExE4S3FXbptXt3Ir2ot639tIucX4f0qqEiAP43EVSLSsiIwWllgF89yjkeprxO4hT46rdwYAqudc+f3UdM36XMucee0k+0PptFJ53ksbTY4hKiDPaRng5MZU3TDZ5srbV1PHG0+7AT7GlB1zHCM4JnBUwCf9EOvP/nDcRHVreAed6g/1+ZWKjx5zzTczzO2Zgd7SrdTn+cFO+d/QUeevbclqoQZT737OQqxK61Nc6EgRAVT2SOuPUXdz1cr2824DWKpcOlXtleKZgDUAs8u7Yg5O8nrAc/2Mubz6x7IWvKKC5MdwDqnM85q23fheTknXpzYLHlr+FgfdWo7PoMZfeH2D0c0tAAbnRnTp0FvPg22OtRBC3/GFggf/oize3M61G0Xma8oXyOHHO/oCjSfRMLNI6yTSnJ1dP8+zSv6o4bIm7LlIfQDEGvzqHmRRDgpPqU6eas+VZNcaHr01+/9UjMM16LZs9rPS9E+b32jLuRFtGXpbS0vun5fovXTVJeY8xNaTULWW0H+ITQFKWbL3V8zJq9oQOenu5twDuZ75SRrGJRkqs+XeQKWdO2Po7UKP9VPa84859PZQen41/kXSiRdAqCQ70OHcvJRf3EvXnMvMwzkXvHqdJ8o0kr1I/fLx4LZxeAuhNQmfKBSw53obk9J8s2Nu2UJBHX3juITTXpI3lV780Nuqw2UtmMY7rU7ks7D9B+nPofO98ZSdcHZ/V5WvC+mCossoVOq2T+ZV+sXEoswd5GO2ejtEZ2Pv6m53cTxOFlMQkHdMhfIMKtp1nKH3Dq7Vn2/FCWb00PpsDaY5J83aDolPRozijFlcaxDBXWdbP58sOzt5r9mFl9CCGTRlLpxzHXXbS3bvSXeFdMv20rqcWabJheiX1kvfro3H+sm2euuaQFkp0JdfAoE2SmyuDjs9Em0GOxFNRPQTRPRD+vv9RPQxIvokEX0PEb21JR/ufajz2/Wek+nXzo/Sbn3w7hM0Zxbe8v7mENXuMKjtW1PNPQCL15wL0EccvFGHFuWjVd/lJj/WXgXiaBAO2zBcW4+c39IZSEQ6f317m17n3ey0RudI9m8E8HH3+y8D+FZm/h0Afg3AN5zMgQYfJQO5UwLdp1FxR/mo6okFs6j/8vlWvW6cVUsHFipQ9kiAc2I2WSPZR9Frll+nFFhLtba3ANaYhSUj10RU5cvmmVeQc6qj75bRdYVZ0bI65Zl9eQtGZuV7X4AzAE44sdr30p5fu7ZGbR6nPltoS117abakX/bfcT02gZ2I3gfgPwLw7fqbAHwFgO/TJN8J4A9vyWsrjaQxQztZCP1PMza/9mmlpHXQ3kss5/p2tVGxfWX+uDm92qEzD7iyMGSqhtJ6lJ/PHiIxUizDZsfjEfNxRkpRV7ZJIC7x8EW7kOc3kFs92/nv8yyz5XL9WTpNIMJElJevym1kWoCrL2lYrQXr1B1b6+FexzqIlu9sdM/atXPoGqDfmscI6FvykGHacV22Ouj+KoA/D+Dt+vudAH6dmWf9/SkA7+3dSEQfBvBhAHjnu961bVjtFLUOoZHUXHsJD+wAdXG1pDQJZSrzcDisIwnVn7VU8X15+f5RJzBgcvXuvUfes5Q1ySh1qetbrSjXOk7tqC3QOubY/pGZEvWzdUHb/F7rQZcC+6EMYUu+5zCfNk0vn3PppGQnoj8I4JeZ+ccvKYCZP8LMH2TmD77jHe+4JIsXSGOgZmJG4nr++FnSgotWYEf/6UnRtoY9j32pnjCK7IAzoLtzVV76TFK1wvTgzo/aYTO90BG6x2Ik59IWyf7lAP4QEX01gN8C4B0Avg3AZxPRQaX7+wB8ekuBD33sImQazrf5Vz+/a1FWWdtzDbW2KTObKlCGIHtaQedhKiaRilQfd7JGUxgBHo07pJuIB78ZywE3Lv8XVTst0etizpeO56Q5F6Drpt95ZY/SPJRpnJTszPzNzPw+Zv4CAF8P4O8z8x8D8MMAvlaTfQjAD5zM60qfdI18Hl1KjCVmO3e8AFXCWFtnnc+RqnyWY+Q2lbWdo+6ldPaer9TRygtql5t9HkjCZMk35jIHeCP82pLtViTliK5tSlzreR8yzv4XAPznRPRJiA3/HVep0QmyIbk14Przjw/qDuWh9LE92nXOoK92N4MWLo96YwjzpHPqL0bh4S2LW2Lh+6jKppo52JLX+bKz64uPgBthvr2jrqZ8ISr8pU7BSzSDc+msCDpm/hEAP6LffxbAl55d4hVeWle9pZXGGVy7Rv8ZlTk21UdDeuzuJdifAS2bv6g92Vl1r7zqnSEzA7x55AOBKCzKsvHxbKu767lu3nnIzqDyzwOtMpX58xWaqVgtVQtW7ebz7lzG9dT5LWnH7/qMiIIrqPaX0ouLjT+nYVeJi036OFTs1VWJ3gA9AEWKEiEoIFuGZV53H+1W4tvLkF+e2dYwk0CEECT/rKYrpANIP/q9CcqBO4JlI4mslaABuzEDf25BZt+fIG2rWxPyXeFzg/T0u7g+sFEMoLw423478Yuwsi7a41ALcjtXEdUSdDRFVb5AANBR29PAE19H1dXx+8tJM2W+erHxm0AadxQpbaJan89rAT551giaZ7vQhbs2VHXJ/efmdcrePtfRtvY8l0r/FyfZASzmqK89+q24cmogqovRqdYmHUS9LsEv0zTl60vnXj0XvQTFFOmeI+mULO8QAqYQMJEcg/tUqn4IIv1t3/j6qfQRPNob+b3U3uHuQIlIaNrrwnaWatzKWxe61I6/Nt3e4hUbqSsBcnzH6WtLefN41LfTy1h3VgP1M5qe2mSa1dolI/Dhss5B52Ly85+T6jmNl+xNGrt3gXG4+sCDuPLWLZ+jVRM8c+i8T8I6aNYk4GPY7qecsJeWcUl+wDqTfJGS/Rr0qDzWOj94AfTaOy6UAUYEBJGqJtFN0ubNIgAJlkm8CHGtJLtt2Wwz16ysEDARiVSfQi7LPlnqT1O254Nz1JX4OwG8rDzbPB/K8ydNn5eYemjT8m055q9pr4+Y07U0gicFu7cPL6VLbavFtSujfVn2UvLWNjQvbGIv1b06bddLXHzJuzdZxXvnc5ANGsnelON/t+eKfU/+6YrczvVB9XwWVGMhwO2a+Dkj97Waorx4Z0sX3y144x+a1qffvfEvjoouurTVhQjIUQ4GLi/ZK3XaRdTlvDhVQ2zZEz9HtOvAe+njAd210z3oqQb94hFVg2ndo5VnHsLW1sTxbVnYj0vPab/vYL8C9bWGvj2dqecBD32bvS1LhtqWy0nl6LlmnN3K8eWFQOtAb8DeUpHqcBqM1dFAziXNqka3cejtRulcVf7a2shWeq3B/lzez9p5xQtuLh2j9kLLufWhSZ9P9sZ7x9xgDn1v+M2Pmq+p9T6YZvGMnSdnr2qbgkM8DM3t5THOf+WuR3zX18j7mvXr5bWF2dzEslSvJ41sdX1Z+rMnxXtS3QPX4udPfbqAr4C9VOkruz2sjAacI4iXpvYbSW/U0NtV3jmtbx1cXVlLS+ZXaoeGioOpraxJrXatVNK87D4/4GQhoSV/55TLnm3FDiFL9p5zitULX5xztUrvh9zUZ+bqaJLal21BMn4oLmjjSKoUVLrrvWWUwUXO5T+4Y3kotkbKrVeuixVgHvvluyD76lq0+479u+b6/ZytJZz43RKRq5P1hdwn6vZB53/+xv007RHUvtttz/j0anwoykT25HZo9RpR/1oP3CuAJ6ys7NEBm6R33a4IS3ksA1hemEGgFBwAyHnhA5WxduhxmqYK8EkXfIxzRIpz3o01O+NmGW6Lx4j5KN9Tilq+hcASJkwI+kdJpHbAAQETJrrDRAc54oDAAZwCOATE8BY4BBBLWzEngGM+ghkJ0U06ysMLAmyWnWlnffJEVIbjrG2D/Jo4WipMnPR7VJYk5R/pbUh0yO3j82E9NwJ6xaD7b73Kq0etlpMtIpQuw8oMWxZYg7Ypl7Rtq7JX7inNvJmZPYPN3sRQ+cZjbL/Wjbri5un7IZlNhhuvNbFejPq3FlNLcj3dMCfXP6rOYxJU8ufSec0+79jkSaV9csNsFdd3Y+RUl1xJc/8pIlfAyTbsxgVQRQvy0oj8g7mjQVg/5K1H17WJQNU0RdY7BfQm4YsUraU6+3PdEtbPDc+3Q47V76qxG+dkaRnfUqPuWDOmRuoP7sto2GBWPe04O+r6enW3l3YIx8GFHtDOxvSVqVVCVh0pzHl7pex8i6Kez8d2bbi5THRxBRiMJqrDW8m0CAN/43nPIbNkIbIkkjolgBhJpTinKBIdDEplbTs4VTZlcLvntk82Jcrmjzll1u1rcSUYP92bfTTite3jtffW+kWGmuQj1OscenLJTs23mo/zyWvMKypYc20t7VMAXUjYVvGAnyDrMCbFO4tT5AUsu3PW5RO8huOKrZitAT6Pq5ffpT5JZGuOxNMlr2CDZRbqW7/bBDRSutj9avFUtZHlrorBywr6hfa0pQ1RA2stpLV33znXRkDvjYCcYkSjaz7O4iH04obenps7jmmpi4jjpv/yuh0HQHa+xQhmVhs84XiUVV7necZ8PKrNPiPOMW+1LKvIClhNdcwZG2q4SOPJPibZidSPYI4xXT8eUInOACcAEarsi+9BDYQcO2DSmWrAE9cC2pTchaRvmoy5DAG2WG9B5IFxrb6yLtWRbe1ybr3MLYB/jH7+4sB+m0SqRRTAb3lZy6g0i45jXVqaBdgp4Xh/n8F+f3+PFEW1T26FGrBI9JTBwVXe3p6Q5aBl1tshTDiESee0K57YPO6z7uCaMthN7aZgzj/HwEjUeGIgMiECWh/lNxXgvephFyTAxvwLAvR29KNu2zWAnAua88O5m8jIjfev1fWxAP/k89nXVKE1deVaqszl1BjfZ9xWqZTFel14+IokdnHvMSHFWT3vc5bmKcUCdANiq1mgbi9LR2xqeFHFTcISBJDZAZJXutF7IWvRIxCIg0p29RUQgYI68hS7xOovYM2vcqAWhpTDbhrb3A8W5ppuZKJb+pW/Pvq9iJForvlytpgI5zKfa0Xc7ZL9LLJOcMbLUrkkLy3Lzb7DyYE8q+/394jzjOP9PV7p9/tXr/I1W4nG78VmoBVJqucDA4mAFEG2cQQBRJyddxm0+oiUEnA8ShkK8mCutxBABxnOO9ABIZCcC6T2N4t0JyAyMJstzsWqWLZpATZbjciYEzVpNrT9CRv41LmWeqBvz52vGTwd3QTYWwidD6lxvifz6CYY3bkpx5wud0+iLNTK9bZ8yqo22zHJkFuR8HXcOzdOOpHaNSSy3FR1uIKTSSR2oS7sKmXed91ZpgSHJH1Ec7VxtZNNAJAoK+JOY/DOg57jLbv8YGwye+jtPl+/Ycs7ldgZAfntaVZUexWrOnktqz0uruV7O0zEfW+rTu05rllafa2px6Kg00zmJsA+oh601hjBKP2oa6zHa6+Beq0WQTqpOsnIBdhYmeYgsw7OZtMyyuYOcwLHiHiMiEfZ1ul4f0ScZ8wq0eM8Z489kgHWpHMdueUldwAjMIM4IaQkATMplhEvp2HE+1eIKYrqDi778XEABSBgQkjisAtBLHgdmAMATAr6e6uj4Yu8cuNVdavthPrtrWhEg7cD06pQMOKZ4TCnFmCZ6agk9++T5Y2vku8mLYOw+rhs23PEdhuPu+UpZoAnBzuvVqa9tmr/9HNfctJB2tWcVu2fU+xmeW7MdIvtvuDiWboXKZ7ckFs+Mlf7q5naXinHZCW1YTWaIqVSntrmEpobwVH8AzmCLqDssJtkD3fTDMiNCJikDw5wpM9GFYYNxFkHaez0IqGNttq8uQ1MYlYgpnVJ0FHT0QA9l3NCqnLhFPXR1TNLc8dfFmk9wxmWNaZnl+yjij//8NoWY2LtJVs3r0NyDXR9EuCSSfekYDcVPpWQWR8nnz3lgA5bsUgbgg7DsXrfdfVaZRDpKJ72eZqAlJA0sMYke4wR8dVvCthNQlomadIB8wTMU9UpzUdvq9Myymq1rOGyRISUdRB7+qK6m5JjQ3cWLpvYGyNa3gBsp2zsXrrRfaOyJOqwX8bW8kdp/HFr4M4aPT3YMwdfnls7v7i+9qy9a2e3zSn7/JS+YGq8Spa2k/TsO9Yi2dRFB3hvo8eyIk32wruxXuLWT9AEyzJg9jhHIM2zQIkkPBaQ6zFGpPle7HZ75ETCNcDgOInUilEuhgBQBGiCQjoH0OQlsp1drgZP887lmklRG8Yr1/rDcOc62NrfIzD1xverEGeiCuybmMoZgMdKHdc10CU9r2SvdNf255q63z9dqUFwcB2qalsquXCZnEkSe94GXgwpd7L2o+q56/iyQYNX4cs1gUxROwmqPicZyktgzPcBPE0gZkRd884PD6UUEY/3YE7FYz8FgIOo8nEGY0KaZ9EWyKS3Vjp70yVC78BAxARkn37Qetqjk+ggFsADiTkgM2uyQYAafKBy1Ndko/XMUn521m2VuvrKbSSFLEjIQo1hv4X3yeSXMl8BYCTILERS5k1ar/57l3dPWg875g7MJR/3ks/qks8g2f33pqYPVd15CUnv+EBzfjttAfpI7deOSAFAqp6xN29d6sYVyG0YS3PIqnoNglJ08ayXzpJrlyLSDAnUARCJkI7HsjSWpWMGkMDxCCCJ840IKQWkw4SABJ6CaB0hgNJkN4JDEv8aGKAAEHAgZSY4gBBEk2CpbWCbGxYATvpIAhzK/gRhlomAPFbv+HCrKZH6QkxLqgBvpoKXxOh0FAW3PIYAPK/eo78pAEE7U95RF7pCEBPMRZoB3+kh9s7h0+R3i4oJnMTMSjd9WrBXnElPbbChejQMN0XtmPG/z6f2zlG92nTySvvwP782S5dV6euWv993zfVyd4QAyK4yIxKJDQ5GUFB6MAAJATMECmVnGiQRaZyiSL+UwFTi5xlB5qaTBe5ozL1KdBs3MJMijxf4Kuel9YtUAzMQStyCb4Ssyi+w0HlnDuhravdIhZf5A7aiDyOEpSmRpy1rtS+x2UfnWoDXeY7zf1Y1/jaccOfU4RynXZd/n1GW9Ppia9dHkfJZZ1WMO3Cbne06C6mdn0jmmKd4hMXSV51T7wsEvO2gc/XzPPsp1w0zgTmCQ0DipOYKwBMg4XMTaDogEDAdSBlKkDwYMj2XrKr6EBqgB9XkRSVOCJyMqwmj4TqctGe7rzm4/Pp8Czvd2oKWq/suVvMhhvg0WTUft79e+0Y39PdTfoQX5aBbVHKt0t1rNMRoa6OftNlP0eK+fsFVJytCtEkjn+wgy7ZxcaKV/3WJ3k7MSXtb12qGpgq2+eTreoMKZmEp5FMqAwnIK9WwMgjOUl0kOwNIURgLhagPEwCaRGtglfCMxU4+WTV17Wh2qvGtXBd2PhkH9LMi1lqgmFaRtQvfPg0D8aoUvNlQ94kyhbcu0//2Z6oQWl7WYc1Dfy49/Ti7mzRwGQjLSi+jIlZ/byJaZ0Id6c36gojdb1M9uZYg5ihKiZy3Xu4LCqxAogpTno0ms9MQdFYbMZJKWNsGSobgUmYQgC5TLYWLgs7izU+ckNIMsDqUtFPbvTLlFZiiLE2FwwSEgJQmxMMEjgFIBwQixHmWet4dEaY7hLu3EN5iYDoAbx2k89IBzgOhz49qP3obdYAbZQjZcFFK6pQkUj9IGYMPw05BBUDKTLJA4KIwtF5uhmorSEiUEJjAJPMCEigPb9oIgT3DoucYkBvTIfcg7QTJ0ug93H6sDzV55Hw65z09uc2ODU7pVV6t0qF2sHH+T+Vne3lwaqn22XnqcopsJHarVxod7iUXyWH/6yARzdk8vqg/cI48G+phXw1jKF46mNqZ8ygVZAV9mmUSjc2wC6o9hCBLUiHoGnRMYqZzQiJGIhH7iSDSHiRgRwAllv3jpihMOemwXEqALtDFSNmOlao7EFS2qX8Pbu6b09zsS5Gzvd7jHJiMHPDjfR/dt6ltRVyYhTn7wAxOYpUshsIacFdZ9gDpJXqrsjeAb69VTXWCnj2o5mzKOtbgfENDb/z4lvVyF7msFV5QKeUt09Vq7IkVYzU2voy1a8y6i4AzNcHi0LlkDkDmycd51i2ej3mr58Qpr1cnkl2A/hYdQIGQKIE4IJGGzCYJF6Kg7CQEMAICM0ABHA5ATCA6ACGBeQIHxoyEWUNqowI+GfBtMk/mV850gcJeUeo94tm5SFSBvQoY1tPJr4pDJZ1PX5iJ+9Npw5xkgw4kgIL8JtKAH2VaSduTo5u7YO9Ih+XarmBDd960sHdG2jbF7ClCpO2Ja/TiwN4odUIj1DoJcB1gb88lO4+cLU9N5j2H0qmloW1SDOtqsswN0CWjXGMpsXQOCc6RaLwYBfSzxtiL7DXnFMBTkEC5KSAFka0pqmMwiAMPKelwYMhryzFNSOEImhigGRwYjAN4AiJYP9DhN50d3z6DqrbZZKNSL2vfzJioHjas3oEDsIwpiHYRKCCPPMAxBnUO2Di9AT2/tiQNy2R5JoBkNkD1jswkcUDn9hk9NZpAO+afga6fs/s0nnVZqgdSp8Fy5165lm9fSdsrotOVXE59CU/uG/eTVC+XUgLFevum8nHLUal9t1B/mWG9UToucjBM7nDJbQ2Vkjuv49ha3xBE804pIoJ19VwJu40wx5VzHiYGYZYQVwSR6JHBuAfCAYwJPCVEHBDpgARCNOcUbICuMKkccSdILzP0dNccwJXtRxOch95719thMb9Tjvfs2ycH6ngHGrDYYUdE/Fzda2m8x789jrz1I0fcaOTgHHpxkv25yEO6plP2RPZ3u65cD83k7zECupDkUZeeqheYPOY16JiTqpMJzNGBXWxiK4lMa7Vln1NESnNmIGYeQBlIfiIWaT3PJEtJgzEFkgkxzGLXcwKFoHPjFZAxQcQ2CcgjiUo/MzgcEMMdIh3AIYCDzmxTU4AUx8ZEyjJZziZ3W2QBWIDYjn4oDIDW1zFWd23LEFdrN5d3F5H4KO1+4p61obMtQ26XANzTiwV7V41xnXV0bXn/SCHqW/qlwcvUyTJGbZa55mmqqHU+VxbnvNxLTixqMctsNr/zS3nhydl37RCNV/FMpUc+j6Y80Y4pT5bJjMmBDi5/yaJIr2Din5Qh6Jg4KAIxiooeZAloDjPAslRVMimqMJbQUzNpyoiAnaudlaFS6wHf/uWR87txqrp9z8dmSG3RE1xGyQOwancxiYqt37wT9u/C3evysLKySeHMMUZp85zHGq1c3gR2IvpsAN8O4Hdrdv8JgE8A+B4AXwDgnwH4Omb+tZN5bSnwVB6qn/b9ruNyxu3QSb2Bi6p5t5KXdSgXAOKcdi2AeJ4Rj0ekFHF/vEdSCT/bQpO6AURMc14rHsw6l708YHYIgfO8smoaKnQuS5gQOCEdGMW9X4AeyOWndbVVbkMIiDGW95DEaYWQZO+IRGCKEuZOE3gGMB2QQhQGME3AJCveTBPLphUWUltJdquvMqEQdDiMuy/U+ycy82d7L4Wh2SQc5SDVq7MhsMRuKC0Jk03RM15GTEfEdF/1F8qcsl9HX0ZV985vG3Lb0h9P0da93r4NwN9h5t8F4IsBfBzANwH4KDN/EYCP6u+TRFf4tPnANVz3fO9ap0bs+0lb8fblDNueBt+d/QcPdmfrceOgs3PW8fxHO7VJ9Mrz7D7+d37SCkxlX7ew+Iy7Rzs0ZMeyDVVypoI5BCOiNx8MTOz2kPd1BCrwe6dcPraOuazgNJLQ5e/fR/dj19w7s7xb25wTL3bNNWbb29SjN2xm7dnrE77uo7y2qvcnJTsR/XYA/z6A/1gLvAdwT0RfA+D3abLvBPAjAP7CplJfBHml8LpkL9s6ho1zp+MR83xEijGvQVcAkpDQgJ4c0E1yV2AHiOxoT0SYSOefB3NK2d5Vjplq+jsF26QMgGi5xXNLBuJEJM9GQKJZFr5g9ewzIVDM24GVpbFUqwCKzU6kUfXI6nvPEedt8L59vf4ufSCSd9qNRkxEsgsTMy2udf6dFeH3yLRFjX8/gF8B8NeJ6IsB/DiAbwTwbmb+jKb5RQDv7t1MRB8G8GEAeNe73vXgChutvbht10zKdnVB1PZ5rQQO865+kePezhp0UpAbwJuzzL7b+HfipJsotvXl8rvlTYQKPDm23uag9mzAxiwhONAFlF1dm22dQRL1V/Ix7UM3miRjVkki0EjjA9jN3HNDa7UGYuAnX02tnxs6G0k7Hrxjy6Pj6LO87fla51n1Uc1klG+Ptkjic5xxW9nJFrAfAPweAH+KmT9GRN+GRmVnZibqTxpl5o8A+AgAfOEX/tvXF5MX0ynoGi3cNlUW2tXKy20kClVsou6YvZ1eZl2RJjo1vlb7nLRoJE55HJlhRmSqO7tNHYqvIwOIveFS6h5AuHM7xRB0fQqy8W1DpoCfKUgYqUbgmRYA53gDaWs41bwyHex8cGDPbexqmZYMs52EMgJMz2vvJbh35Pk/ADmAB4Q8+SdY2K5WuDUJWv+SbUzKTZ1EO2t6izFCd6zu6z5hn7aA/VMAPsXMH9Pf3wcB+y8R0XuY+TNE9B4Av3xGuc9ORUpuay4/eNa9zpprqzrCA951Vu2cFdDnGXMU9X1OsbHRC/XAnsEEUd0DSmfzYC87xwLBLx7BQFkqRygQ4a3pkCPrcpHqLMt10c0gWY9JGYR1fJCF+BbAZzD4PeJVgwgN0I1pVY3KyNFs5jTL5tBgLNuDnAoXyfWx7741W7C3DCCQTRNu8y7baC21BmnIhZmBGuirwL+AToKdmX+RiH6BiH4nM38CwFcC+Gn9fAjAX9LjD2wq8VFl+5oDbaCudrHe3lxL5cUVRraNl9cb6dsca4lUe3qzI87VUUI0NaoMsl4cLP6cTXUvHZnyjg/WEVUth4IK5VNQLxSIcAgH19U1f+cH0EoBABIZyAOYasluDkCo1PdOwNpBFjLQlwiHtlFYSPRWsvs2blX1XjBO731V5zv3ytCjPGtpb1qkHVHrCxhRdl52fAjn0NZx9j8F4LuI6C0APwvgT0DMue8lom8A8M8BfN1FNbgmrTGSc5nMqfQ9U79r/i+XQ/Id06T7PNve68WbbXYvg2XWaAgAA2GaIGGaXJamstmE7CVi6VB508YgqjkR4W6yc4CYBzW+iAgHTE4zYeRVJTpNQWESkGMC0wSoisskG0pwkGWwMB1AhwnTYQJNAdM0yU6y06QMwEtz04jU16H7YbTt54+9+eR+Xrr9tnP+2GMCBmxmrkYoBHgTEKYq75EfYI1aJu99Bd7pZ2kvoU1gZ+afBPDBzqWvPLfAy3jSNqpdb/1GoSbteZmfVuO5SeedSON7a0m+qDchO4xEXYauulqhunJ0VXXQNLVU8gsxqKcddQclAJOt325LSTOAjkMKdq/Z5+Tyyx8Z4rMpvLKkU4mcK+q02cvGYIp5wW17dZydPU98SmkhSXv2eg9oPXOg5EWwZTV7ID8X8L30vTrZ+XPoxUbQPT1t8aD20i39Aj3pLp8yc6rtsKWDUV7uOcFWbymAck7oqmhSW3gKhMNBpNrd4U5U9YNK1MwMStU5r/2o+7OzLT1V+yfYqe+yLFXIdWUKwDSpVJ/AYUKYAihMCGHCFCadt6/Mh1CArrP2LTZe5rzTQrL7EY0e2Idqu6njwAL8HlxeI6i0NAQQyyq7W1X2HvXq3F6/FORGz7BSzRUyOcEscxEq7U7I1c3XBDtb0jsQDCRUvu4Bkzt45Y5T5xYy2EFmmVO+S7zhmqbJ09uhrc0ZiBAm3bLZHGsQNZpZx8xFnAqDSQzW/dyKrDVHYFk+WzORK2a3k0h1i4LLy2Hl47ItmV2ba10KU5VPBgr731778NpO0G5R2/W5tbj2gTHX7yQ7zci7z4rjsS7P1WAB9rYfNDY5F/extWmNnWU/PMUEnhzs8YH3e+WpJV62iI7/enHnv66tpOGBy4KxYWNKmgI9X89UpZH6JQAJgRgTQbZMs+XZSDZRpFDGoTkBKWhtuT6yqvOclBVpD7GqShqWFV5ERIKSbMt0IHGWHaY7YApAEBsaUIkH4HAHgFnCeGOUiS3zEQzxhEvW8oxRWUsKAAeS55rE6WZu9nA4gA4HWZvu7s75DGTMOqoDkHWxyqCThmI2I8g9oGwfHbTNQrBwYNGSMiNEYZRkc1QV+GCSPFl2nRf/irkknelg89B15T+Yo5ITWHt1jmVQxlWG8qBoLf1DeFPpT8ECpKgwIVJ/ja0daG0ENq2iJzjGgH9SsK9XZXseWVrYOQ/ChoOy9X6Ug+TD7lXmxC6fpsYiYkpy8vcUxkCuqFqiGwnQZWEjRiCZFz4FWWpKOq52eGUyTBBmoKazLYvMup+5SX5oOjCqsVzSejIDIcmiEQRCogCy2WfTBJpk6aks+aF7r9tjAFoBclKfEZMsQCERfgRMUhFT5SmQbPEcAmgKoGlC0A8R5feQOApQtdISAW/LzEZtarKKqDckwRaPCEE0D1kE0qIKyxv34+jWJiASFwRRntSD1NffvJQXGWLgLwzemFfrlS/32yQg5Mk1pX5OO7Hnyw7RsqT2yC9R+mOfXmub/VLbZpzhJTepetrY32xqa+UQM8Q2zp4gK8CkJIDJk8tIpVSH8bUbSFg9ukaNY4KVbWvSJ5kUV+kbvRecMdvQVw5qJYAmqZ+q13ncv/pYo1JuX8GzwZ+LZOOStnWm+bby9rcHxMhmbu1zf65tU59v/m5vjmqA+/b030d+gbYc/3vkkOzds0avNdivTa5bnk9Ug9jOsQd8xq3rIEElGUsIDFhWas2rvKRqJF8PtQ0rV3ih9eQyHBCbC+BoYaEmxctCGsfjDOaEYzSwq03OQLChN04A62y2HGyDBuhZbciPYf6LwrCWgUUhhDzVth22aheLWKNTwOsBy9v5xriq8Xf0wd+WZ8dTwH6jwd57yFPezO75nqRbpC3qYOUhh2MArvG9O6j2O7m51V5yZ8ALCnz3KB3Z8CCqqnT02sHFvn4GdKsXlp3E8gX63usMsqaT5Xj9FCsJzxR0WE0kusxvd+qtc8iVh+TS3E6F9c7Kwgd4oSgVDYF0swbRxutxcQ9S91az1C/nl/2Em/u4+o42us+1X1+Nr8vpAbenSYxAvtAGOnUxepFgb9WsC3PpA72TTspspbrZVdrg3N7R1wKsE1TPYNIuLIM5CngDMIkanWBDTxMIyMs75fLZHDoAsQ5dBej8kyINS/pyn5VrlGxjSQ30iUni9+M862o6CffzsQL7xIS76Q6BJkzKwEIIuhS2i4fN7WVOJ1X7XUBNgs2GS8UPUrURgDzOLRtYtOPtbOYIauZWAFM/fzEZTktOyo/ysP7YA/XyOXihsSzqt1Lf598k4gLqBTq4Elaed3ltFfBeVZODv7Hcb53DpE2uaK4wrI8O6+08t6IRDDoPUdlnTG1/yunN9jVpWNetSMxW0teSvWamfeluHc9i0xOz+M6IEFLJF/Ar3+Qvua6lhbWOThPxkr1tUyqPmsFmocHjYa6ii3mw++fuHdfIFsboqeRSz3FfbaV17/wWDWArpp5Bsm/czXSFmAkjLDQpT5zvqWx9NW6VJ7iLrju5M9lk7zip1h+kSGBRyfNUUpLxcYDFy51S1Sa2hDHpfUklbEoJkRMoyZZNIZGaBXXnKiujsk44keEvC2SZY8RxnhFTxNEku8yDBR0iDhrrb9gOISCot98ke7JG0U0qkgbskO4TF2xEAjoVlgDdMbK0d6Mue5v9FGhyW7ln3mIPV847uN7kmOTo/Y6k9nL5sTpdT6L3nmWNXqQavy69XaqVRN4ubFO1Ay9rWgRcx6hs+B5ZgmF9muReXc1eLV3kEcUT7SV7ZbtzbWrYNPZKvfUt0XYedprAQrIX1V7U+6LWWIQdq01ra8nZTDiQ7d5ewNLW2e13U96HJBo2sG8vc95Vbdx5j0sfRt82bvMvGUCH+5bzH3or/awxk14dRhJ9lM8aLF4o2E/TVo63hFh7ht1/oR4zyPYuoOqs/PLKcMndunECmt9+hVcvKZZMpKjyRbKPtQUbnvbqeyVVYkR094ag2zHHCI5uskmellvCe6Ouh2dRZY1Oo/U07b04ISXGH8v9y7NpAVi8gbEdYi6hs1z7btp3Phriat/bKcCP8tSb8hNbG7QSvi3T0o7K9VK8pwWsqf9r9FqCfbtqM1Ln3XWufsEg0xMu9tLJUtASpEVqaUSU/84mz3TddNQdpzUy8nBP/u283StPzCY9c+dipMCglBDh7ExmmYXnQJ6qNe2XsfytX6IMMtByzfeiN+iDlWW1RICrZkLSTsIAEyQ0T59nIB1L+Wv+neV9p8Du85RjaVk7ZxJ9ucb8six/bg3UDwU68BwOulVFYyv5YEYAnTyX8rhXl6Y+aw3GHuRcgViAo2GUqirnTlZ1tlKiV5sr24xNsinLICobJJDrzOwkqEl3jVILzDINlpO712LSveRXVV7LD+rwy89mdqRT1fPqOa5pikYRsud97I/wbVHeovgw/bVUWpqdss+yo00uu9y8MDmQ29OYhJ0HbBNM/17s/XLzPZcBzj3PGyFkzIqsihplmKvmbXwudWX3DnRRTmYWPwk4L86Rt5yyvLVNlt3V2qpPzwD2a+VSO0nOLc+rhqhyW8upTl+kkrvGNeevO3WjrnHtmMkbNqBoBoEb1tax06DAMoADOuedg2zqANvuCGUdOQOIq0+0cFHI1FPbB062iIqYY8Ic3W4yQLHFNdyWbEJN6G/JVNqy5zGvP6Z9FPAzOM7mAs/+iZxD0zZLaYjcPjkMlQsDz5qE5aqzfeQxzEcidS+Lgjh9y4G3ZvLLOra/Y4xI85wX6wQ6S1rbc8iNg7Yd04tV43sawpoq0214cpx7kHbRXa0jVXXBQt0mTZuXYmrr4F8c6o5pz0ds0za8xOvUo3HU2VJPZgcDJVy1rCcnzMTGFFugJFec10Ryq2heSZkMMUvce5gwTbY8dVmtJvO9nJtZ9IDNsLPSimR1UjOfFaCu2eHtcYtqblQPOXrNwzlLyR+pbj9Xj7Yu7Xd/buSNH9V/u6la6MWC/eGkEplK9wN68sZfzXdqWpsB5QIcRNfOIGROeYjJnG55TfisHi/XHUcuwx25VgiTMy2KrTgJoO+0PjoP3bZjnmzRCrOfnXRchJaaxgvf0UVdD2HCdDggMGPS+obDHWiacHjrbbi7u0O4O8hc+WkqGebn8aowsrSuW7nY8MYNEkeZbDOgHtjteAosbUCTT9c63FpTyKf1oO0N540Ab/Pye+AebVU1jilY0hODfQmcq+S6yJJXz9ff+tfLbydVeZnWpHOWA07CF72h2H/dl5mlfMdxo0cCOlNsjamoSuscdKRzznWRdhXizfzx5mms/t3Ht3tBDvChSi+SPSBMNje+tt3b2ovjzT2f+TvADvuNNsO992qXlxK0HNMClLUvxYBTJLrwbgO31bIGPDNV9Wn9MKPx/l7de2Poa0yqzzT6bQO8VpJ99JTbmYv07WKz9eXHmGEV5bQtt0xrbNW8bLtD155z4Ld0VpeQc4O75sCmTjhb1AnMMqfVEMLIjjx4271Tfys3L2YxEYh1imoKumGE3JWH61SyT2+9DYe37hAOdzgcDmLLE6E4xZI+TcjPZePu9nzycTa07nGXdPJPxbibHl7aTupXH9t3syRhOm64sLpWalkcpvVQmV8Lz6+iY2lO0cgE6T3rOfQM89kfKNlXbh9yTblYn6TRPdbB1jWDxYxn6qWrVcGce8Odi0RHkfLNCxZAtE6pIuaK5CGZ+24rz2pH172U611Qs/Hpau7KNLu+aATOq88GWOeNP0wI0yHPVQ+6kCTZvu2+zi2IcpvmSbLVM2bvtTFDa81GWveeoz3XluuP+c2ZI640brmnkvYl71ailx13lyr6oh4t4+30gfa+muWdphfqjV9nGpuuMVDGuN1VJ0mdGHbHIk0tjdnMdp1h2GLdrMH1CsMYWUrOQ1t+/7OY3FARIAEoKk0oS0HnOYY633RKLAetp3qdbQEKIuh6czIxxeqWOJVFKly5QaVvshlsugosOABBJ6dMunbc4U7AfncnK9KYRCdVo1PSBSJ0GIkMHBF5vzfnaScmBA9mdgB05y6RgPmdtECu0iyTLBx1gQBV5W01mxTnvGLw8XisJPxpH0BNo+cwpk8t17wdm/3hYH8wyO03swK9v0Rv1gbYbFmnDjc06bCWVwdMCFlYq5GlMxVYVNOlo846h+WXSFeAZahn3eUFBTFRM5uNAaS8cyu4rOZqmzrm1mEJkDHwmLoZZN5arkcgkuWrwCBTww865KZqfDgI2BEmmAM06YobHAXsDNYjEOOsLZaqZyGmvL98ZsSsD20z5JQ59pxhLdXAouIkADKi/ZsqIcn1/S5HqeckobmJgJgkDiDORxzv73VmoAC/8m9QMaPWNtD0TIncOeb6uIVeC5t9zelx8t6T9zVMokmrrpxyX6tOqgSvXoep4gv1UHJs7ejMQKyjmyNLHVzaLVEbmaqGM2V9r1I7AZ0+WiQS5wAODX+F1QfVc5l2ERSZIUx5uI/8+LpJW5ijCrJUVSSUlWi1XDjPe9ueuV2L6WDX15xZddsuV4nx4GnT2nEdSFJ3CR2u9+jzu/yklDDPc3kml7eP4e+VNdIAehqBxU6M6LUA+9WoYwSpRZ0vL8mcbDWTMNW+3K0qlwM6A9mDXj41v7DOG+CsCbm9LJ1I5ZhtbBP56gvw0tnyTXqO1PzI3uMku5MSbPdUDSIhDeO1jqqbTBwOh2yzU5hkOyRm2b1VN3aceRa5zaolUQDoIOVa4I+BXBeclJU2LbLQoss0Zev32KDC18FOHaA037dITAGyqOqmsh+PRxyPR9zf3+P+/j6fq3wwec3+cbShP+/7yKiOo9gDoxc5n32Uz1re3fToqf7ZUG+ENOdL3q43pb1om94mMyklKGxfRn55+SXap35xrMBhlc6cZa7WMUt01xlM58jTgU3SN5KP1ZvsHEv2IZS8vOZbvjsm1XRaW9VWRhgIiSMiSIYCdbcqBI0WtJVjAfiJL5VUN5U9t3+tBfj3JAzTny/aTsvRC1bYpeuf6+3W2jrl2p1pvLRvGVS7y0xPw2hB3aathjaZO/250IuU7NdhGMuOUvL110ZpTD338eyQWVwk0sdT6+TxalwIuuKqDosFBMQYa8AnRiKW2HUZOs97tVmf9Lal7SyaPQhmz/pnsHNxLvHvfnNEhtjlRLKuPAVgIoRD2S7Jtm0iN09dbHTZZvo4CyN8lYDIBEwzaDrovPaDMg5hK4F00gux+P/ACEn9KmajS0p4sBOVY69r9DcYNr2oerv6jlEd6+9lKA9AdsLJmnxyNIn+6tUrvHr1qrLZre0ndV76bahaKd2T6qNPp6ILenkTYbIquyLFh9fclQ35eNuRFmlN0hQVsggJEjSi5spoXsziPJHa30uVLA89aX2ygDJzgF15pQRnFjg9JDMQkagW6+53VJFxYVLV2oagIPu1WVkL6WJ1KVIm6d5sMTGi1oNAQGLQBLX5zecggJUNb6xtvWPUQN3v00Wb6l1z781J+q1yozUR7Ljc1Sd1bffWG2/1PeWh3/rxTzeiZ5DsD5XKp9jFGOjLfJabFFbS0B1rbcJtFGCpnGoPp2o7L1DW0sMkQJkmGZw7JN0yKQbEg6i5wTltcgezzqnOMwomxcnZsToMRE7NB5CXk3CbRibtgPN8lFlXc6w6rVRfvPFpmkSyswy7MUsMPE+ygmxIEIkMwpwYRwZiAl5FQmTGb84Jc2JgugNNd6L2H2STiGmSXVoOkxtxYNv4qfgUAPHSJ2cSeadbG4G2RhVAmrQ9Z99o7bdWotvv4/EoW3DPcz7ny7F1AHuSvQf20eaTC8CvPPYTB9Ws2xRbcwG2qfKrNrx2JDv2qlWkaS3RKynvHUOldt78rolKZBYFQmDdfomD7GuuO7W2L7FoJGI2EDVj0ATX+bVe9hRmV7oO7FV28RjrXPVYprTKY6hDi4EpMGZiBBLvu4zNK9BQHJXRJHlizLP8vr+fcUwMmhgISSbPJEYIhJQm3VhS93dnQmIdwlaGbEwUcJN8HOC9xPU27Bq1NrQde592IQlL25Pgo8/Iz9RzGvp375/Xri13k7V+ckNq/DVoC7tYB7o/9tNV9rsCvnjl3T1Oc1Tzc+FoKaVROVLptNM0yUs/TEiHAyJFxChj1CHqXuScFp1TZqSyTLRhUbojyZj8ZMZ843yz1WdqdXMuC1PYstBZsjNI7f+UAogm3aYKmOcybBR08woG4RgT7hXs9zNjTsCr+xnHmIApAkEj7e6ibDB5d1CJPoGT7u46URldIMZEoq5YaICp7KzPzjrmXZgbyvf8mst3cwCayeHvq4FtZo2ZPOzSAylFzLPMRY+6Jl+KEZzs4/wNTkNhlp1oSDfpNCadTS7zZRDJ0GjQrar0sgybFoFgQVc3I9mvQTz4XqXZIvXXrnkPjX2t5XYltHt5eTu2TilSmLhw6MlmhcU7JB3GijGCIjCHKEtOcSndOnJiYS5Rx8RmyxsAk5oaDuymuhvYbQz4eJy1085lrnoUB5ZE18rqMCGoZ1C1iDCJ4w4QsEftz8eY8CrKYhf/atbjqxnHyOBwzGCf5juEaUJKb0k0Hx8E7BMkeo5IYvJJ/XnkJbz7YszVbP1K/a6BbO8ypf6ij8ysgTH1LMQYo7y9xlSomOZ8VKY5I87SzrDIwexgVEpJ6pukZy38EFn9I/P4qnnIJbG/ntvhViR75qJXyap8H+R5Uo1D2zQO3F6yd9J0FDJ4YBeHnTmZSiSWUc8rb8D3NhoHCT5BzCPrWToxgMAJKQGgUIJh1MNt0iQvgNBxKLULaGTTxLn6TQLmYaaAXFZZZUd83LKEldjocY6YE+fFLzgwoBswJiKx94PY/tPMAAeJnGOZjjvprqu2Om4ICU0zLpye9VpwtROuVcG7YFfPeQt2u6+15dvZbdr8i/fr+6MfZw9hqQm2jrexxljU+FP9/cVJduA6DIM7UO/l36ZziniVm6WtZ5zrDqcknRfJ2ZiobS8iAu4OoPiWkxQR8RAlx5nBrLvDRLk/JXW7sWwIOXEA6y4xk9l3pnL2JHulzpcIMPOEk9rN4gLQfdGjBsCBMQdCCsWOjBDP+3FOeHWcMSfGv7qX46v7iGNkJI2co2nCNEeNIIvi7IsHHKaAOAWkQ8AUCDgEyIrZk240cYdAqQOGeqiqB0p/zpxn/lxszJveFNW2f5g97u+z637FntaP0DraPG31yp9Lb3RQDZCtuOpM957m/nF+7pg7RznnO42n7FV3gRYckjiw/AtmdmU4JxJM0uoKMrrcNFwHZTY71Xdezvn0yAWVdp7TpH75xYyyxHTUeP/IspFEYiS1NwMABJsRNgMIiEEGHgMYkXSlHrXVTfqFlJAU7J5ReidV97000rvVZraAfS3fWqr3JbrPYw24PSbWA75PuwX8b6xkL0pwlfG6MZ+T6EtFUdXb+tkabaa+E7nUDhyAe4HTAeGttxB0uCaEgHkWWzEhibrOgK3HJs4itcmhw1Ikx2madECgrJCzBPqCh+X6AEAgXYiCRJW2iTametqSU/65U+IcRDLHhOP9jDklHHXoTTZlDkCMoJhEoiNiCgEcZ9xNE+JESBPhMAXw3STbWaeDaAE4ICSq1GCrs1ffPaDNP5Ftb2a8evUK9/f3VbqehG7nn/c0CPvuHa7WTofDoasV9LzuI1oD9hqjaOnFgf0cib4q6VfP1aCv82lseV6mLZLGgYlt3LhTpoErqCdqmrKH3i8QKZ9SLOAAbPmoFzd4ELq6VibKQMPIUtLZnaGxQXMILVwHc/4OseFt80cuvgIERLaxc1JwBPDEOMwEYkZgwswEcMCBAEyEGMSASjHCpsL5oSlf99GnDXxpGYD/3YLd2sNmCsomkn1geWnud5j1bb4G2B6tSfCtKv0msBPRnwXwn0Je5z8C8CcAvAfAdwN4J4AfB/DHmfl+U6kviC7RIgx8zNBtlZDV6zUiorxemznrpmnSzrmcMCGAsumrnIEOKqp/yPrFSrnoSyyb857tSyAvJjlNsp1TnlZLBNL17gDozLni9bahw6Sx8sJ4gCkEzBNhSgEzMTgFiK5P4BgwwZ6dkdIEDkdMKMEozKJ1UCAwJq27zLATP4TOOtOQYPNTHI/3eHX/Kq8dwCwr6OZIQs2DtX1tf70wWRllPoBffstMsNwPOir8tWkr4zgJdiJ6L4A/DeADzPybRPS9AL4ewFcD+FZm/m4i+p8AfAOAv/bAej8/udGLh7wglVs6Fi5nEi9trbpsWw66jL0H16HaLY28ag7olmkQrxpbh0M1ctPX25uwXpPsRWWfnBrvz4dqDn3LiJYf5Cm0Nj4OZqQoY83zDGASxyAlAFPAMTCmEHCYSK7FKLa++wscQGmpWuetpavtpXW4cT7ieLyv7HIbeuyFttp7mKBgDgFBx8kpLMvu9okLr52TZo22qvEHAP8aER0BfBaAzwD4CgB/VK9/J4D/ChvAnlbru/Vh2vnhcI4r775y3xZZMxgR9Uo1lp+XhsUzbdfYpKgdff1NvUZRtQ08BMrj6wJCKg4wMvua896H0yQhtdOBECNAURmH7Z8WtCW4OOJAkKmlkOg2G3c3V1veBooATEFCb6fg4tFL+kAy6UVWtiH1BejEl4mAwwFMZcvoGDT6TTgDaAoSEkyEQwJsNd7AWm8CiBhTksk/IdmCGNIAIp2lIe+PEpKLMAOYhJlNAQgTItSBF6YMOFPHowH4qEf9nWbZ1qoKMrK0Keocf6HsnwC0dvohHR4MZe6ArasHonysu13pp/bKuHyxXlT309yn6sUte/Qgyc7MnyaivwLg5wH8JoD/HaK2/zozz5rsUwDeezIvbIHz6RTL5/F2tEZU22YAblhsedeM5a6y/sXZGSBHtTCD8vIpnEUmZZVAV2HJqrw8NXHxbJvtPU2TdApBMALZnG7G4Y5AgXC4E/90jASakcNNAbPnWSaFca4REsmOq5mpaN0IJPPEA9TeDALKAIBTWVzS2el3uuTUlI+TztgK4MOhGmGYU8KcCGkKsh4dAYe7AFJVPiLhQKaBSJmAbPFMTJjoDmEKIDoAKSABOM7i+5hT1DH2GVAHHU8TQoiSPxGmKS3APtsc8/tXSLEEEsX7I1jnAkRNMzdLSGWnWxAzgoLMFAgUMFHIx8PkPeQa30+i5vs2Re4PbihQTiq/bpaL1mvVuRNoX7MUt6jxnwPgawC8H8CvA/gbAL7q1H3u/g8D+DAAfM4737n1tpO0AK9izfPGHsCH9+dsnGyj6gY52DknSW0jhlqbKOf878IUnCc3/1uyQz+E41VoICEFQuBQaR7FuWYV7Tjqquct/xZDPLadFLVBILI1s+3Kal5wUWm52NMQGx/G2JCVkqyM5DKVARKKD6ElkfRuko5pFDod2M55sGenXOyvzZ9bqWP3tsNj2bfhr7mJLItj42Px9StOTdZ3Vfpd9cAbnW9baIsa//sB/Bwz/4pW9vsBfDmAzyaig0r39wH4dO9mZv4IgI8AwL/5/vc/npeilHjxnec2a35HVrKB3qvVG8hes4e6dSaz3ed5xt3dnUSs3d1lgKWUwNGcUqUDhw7Dko4mJWaHEiBKKRX/AKABIQTcHabMZAzsh8NBwDlJ98mx/dAlqjR4PmodJa4cEkPOeTt2CWUlnQCjz2oAsvqWtpVnizFi1rqnnH/MdfMS1IcEe3vcg92eNaWUh8xayT76TNOEwzTllXr8e2vr75/BUwt+//2UD6DNR979A9R4iPr+ZUT0WRA1/isB/BiAHwbwtRCP/IcA/MDJCmHZAMA5jod+uvb+td/VNWrqU4T1hvKziHfqVh2aeQ7gS9pyj5cqbUcDigSx8e3ccVC/9ArweqIeAhpIMpPuTT3kuzmmyAFDPezThGlKFYDneRJBlSU753qxhsS2ErTfTLoJZZD1433dvHRsh9p6kXALTcYxOitj3A51m5S2xOK71ccfL6VzGEBLW2z2jxHR9wH4hwBmAD8BkdT/G4DvJqL/Vs99x0U1qKgA6DJ6oOKQVfL+NTlwlWaJ7VN1kFlkvit4RTufo3qSjL1gA48MHx3BLI4l9uopc/aE2gy93EEUE1Un1TFzH8Jpnva7Q6g6tkk/CgE0HeoOHiakxJimiBBkyPBwOCDGhBAmmdutEj1v9+zqZhqB7SxDLTOGzjJDAiVCCDMIhHkxLFls4BTdRBa2HWiF41CQNW+mg46Lk2ooiZQBldGG6SBBPaLpyLDjFAIOdyLZqzfsgN1+710rz9Zf9qp6fw+gTd54Zv6LAP5ic/pnAXzp5UX7jt1e63H1dekt51ZKW1zs2a/qIe6l8bq2H5/r5bXlxZgEyz8pO/Xyb5SgDFOfCUA6HJYTV0wtZ4uDt4U5GsVOn8Ori3l56SaWu/gI6oURJXJuuVPrBHEYZn6TbPqrSeOA2QE9KhBLsFGQ2gbKw1m+LaRpkw5n6lg3EUidkfUraAJquJXsqlkE8axLneWNJCrl5iE3WxvftrYK/vtyv/jesQf4lkZa6QjsreNvjW4ggm4rx2oBVufRet23NOhCvX1QHXUtNNaMzbni7OjWlqulkaXiKo0HlJfG3maPRHkc2aaoRkTnO6hNF58/oJNd1IlWOeJI5pXnnV+dOeEdUyaRgYSypXEQ0yJMOHCxrW1RC7GjVeqav0HBJoex7kMo4FkzC0fmW2sT27GNdvPPbBqV2ec+JLYFXGYwTfy9vbe2PqM69373qGKGK331BsB+DvUBz9kTvh3olWaxWuYy/ahq1AO6HntARwt293h23iLo/NRXQr1MUgAQIznPcwRskYV2Q8PsyCnlhFDi6X0HtuhdD3aT6uSixgjqaZ8k3FWkvkhvCxIinX4bE+se76LuJ5aoNtFIlD/Bv83BW9GH6oGnfe89h1l7rieJe2D3beTPt/eOphD31Pde+efQyL/R0tPv9bZYjnfJccvvTh5VW2iX4DWJXoIS2iOxK8Pb29vaTvJYu0zS+T2oKydUTqbXsiRGldbHWhMAdh2PiPLCkeSAzNGkZWMvpmV7mxrv1VZjKnVMPGqQ+w8ICQxZQ9Lqw9kZmKVblDXjTYUnLgtmpmwCcN7osbzP0ubn2MGt1G4BXzsqkfOp2sczwGYI0p5/BOB2Zt0I7L7cU4BfBffKrS9ky+aByrNBopt9VncaE3Vr1dlQz5zXwJ5CX3X3ktJfs2WH/Kv0nVQWjajjrjnpAhKBsl3sJTunsjR0SknQm1IF4EmdYUs1Xr3O6HT8SgMQNV63mVOGoHMCst2uvocoC1vIcJnE0BtDEAZgsfOonpPUz9CeXwNSaya1be7Nouo9tO/RMdZWwve0hlaN94E6PSecv+8UjRhUzuOW1PjeA/WcDPJQwAhMNeC7JdXq66LsVmKYlC8hiYudWhcmQJHEzEXKt6+g6mCLa5ZfbaK0LzWrlaiBZ1Nbq51lvOOs6dxA0TSKtPbSuwEI+iAhMteffIgsBLh8YHVr28B/Goun5FreX3l+c0Ms4+7tHZzqX166j46j+9vfre3/ENp6/8hPsYWefVkq35GrpJy6arxkYzuInLJ9Tng+UQNTOlyqrheOYWnFDoa7Vjl1+lWuIsPKsxaGYxsdjMi3k/fQA0CYAqAzvohIFjw8TLqxhHuGXGeuAGd7rXfB3lPZydvu9XkJrGEAFg4qk1y8yZCqe2zhxCVoDNzl+f1jLAHf2u8LE4hKII63tT3z9Pe339v34Y/t98emIcN6YFDNVWkpvfvrZ2W1zaWt8sH4hawBvbz4Xn0UeB2zIQvgtkwDOuvwFxwTcSrqUjq4vHO5G8hAxTVge2AEpaozEBE8h8vnHNA7BbprPk0t1cu5tq6lrO3SrzA+D/h8tdHQaoneeXeL51r2qxa0a3Vd0x7a7+dSTxiOaKtmYvQsYO+p7UYFBOW7nnHHseq+1S8g4EpV/1yW2WgK+bvGZld9Tq4nwoIxlM5jJ4OWpWBwjKNfV9eZO6pqC/QQZB562SiifCgU9NQx3J1n1iqlJF75qoq8ZKas6+1573NMdZx6dDa7zBuX/dlt51ivpgNlX3jkGln8QM0QiHSGXN1yzXeGjaVLeLFXw415eWHhmXHI70si7ACiCYAsr90zK86lkZOvx0i2AtzTk28SAaypRjXgDBByr/1f82aWNMtr7TmpDfnRAZV8i7vt5TsbHWxhsv57fTO5B+HKJHdBIB2B1GoilV06fJ6itqbcQYKaB27NNmaAi7LnJfuibGEVIPVjVFKWG0aoaXvqdeusqhxqqazNLum4ecb2fTPa/u37zbqQoOq3rfpb8ijPU38XM0uYQlAmUEJqRyAd0ZZrHsTtuVUTYgX7zyrZ9Uw+V78wA3m7Xusw5+71fsM6Sbo02tsMsOAADUCt0wJ1d8qJiMrWb/7eIHeUS+tDeT2HlJEHK5E46czr7lVtcpWoHW398mSyivfeK9NIKdveUJs7KdMzSc4MXQXGhasmGVuPFfhtCSt9PpQRwhboVpYnHwzj22ZNIq76cjpSOjNSF98gfTYhr/p74rNGbZoe6Ntro+sjepagmvIiCkBrwDs7V6UiuXs7OVZ5jdMBaJE6rBtUCvaZyHIIqGgI1Fj9lHUaUuDrVbeRaPepNnQg8SeU8ojEM19Wqk1A8na31cmBfWC6AIyUKPsJfB4UgmxeoWCHvjOGW0udS0isqey25FPqqPFZujNqFu/eZVDmZH1kBKoeIEaAH7V7NRXW3VNrLAxbE+EUwEfnTtWlpUt9As8g2csCA3pmIZWynYTiLGuf2d/vVb3REW4oJ0u2jkffq3PZgPCFs68MZ4nsshUJ7XReRq3SW8kL77PLq1Upc0fpSfNGDfddgVDAGkKQFVgqCQJV+9t2Lb/ZtBBGiVl3yhFrmzDq/eQ6L62o+vasXKvtzK0K5M2HmpmvSc2lQ3Sp+nrG3n5v8xxJ2B7uehJ6dP0SWr1/5dIzDL2JYySbGETdRudsMdp9vUbTIbgGGOXauoRPzNlmN03D0odWteVGlW+ZwPJBlVuYQ8h1AJVOKdUTOBIXxtUDe64HilMu171iHOWa2JRu7XL3jDnAg+sIPJRcsmoeqncmz6AzXItq68DcOtuWnwRmXUvOLe6Y228BVqtqv33se48WbYUxeNv7rD+2nx6NGM9WVf4UtUyk90yt8PL0TLHxxr1baU7u+jau2HvRY+leE2XVv1k1xurQ6xBOipNkDjLpp1fbu/wzFn2hWWrazvK2zjySZqLGj68BrIs9+jzHjKuvaTjTpKlTltwDSdz2RVOUPDPKhlDT/oXfXSYtR0Cv+17/nlN0jho+unbqGbwm6DFT968x3dhEGF4iZZRuoLov1vFak74dW5/c//Xyt1xzL4DLklQxb1DoSjMAcfs8HYngn+0EA+jWjov0Lfl1VFfIlk6EZawAhaALYBZJV4ArP9g9w+late/iMrv0Oal91i2CqD1/Kv++yduP5WjpGWLj9dtK5Vh7Sm/4IadYkd5bJHvRIbyNa5x/WWZV00ZqyfWxtCmQZh2CYy3X5ZoFbI+B9cGeb+PtnaWt90nJLmJ8WR9OKAvy1XXx9R/XY3FnVTevNYz7cK+MmtmO79vKTE6l3eaIa6/7Y+9aj4Ymr5fsK93g2YNqsn2+WaKvAaKv/vqyfU55C2CcePVcvixB3/tRnIFFafc8gvMt2RXFmrcLIhl1mmpkgs3mXXGMwT071xpD1oY6zFL4kiwoYZM+ZGqqhsoy1Jsf9D3W5XlnnGuZuj5dWruW3HV/bM+N8t3CALx2yO5cQPEveF/MUhM79bH0a07B4VOwW0PwdiW70ALwoM3qe58rngd0n5dvn0UVumDm+qCdejGG3qlXVY+c1hVinHmF87f1HKr6A+Jex7Q94zp1FMEuIJfxbH1ngfP01xCcFuZsRzYmt7FudUXZPWjty2kZa6sV1Nc82Uui4bV+exrzrqP3SlBOn7m2772X9zltk5nwwOdwKq9nAHvpCDYsZHGnJvSW46B94HppPlbnvTSv8wlN1qJhLIfD2nvJ1atxKVZMIF9pX7JKO2rTMufFGNtnzWXTMgSmZXKjd75Mp7Y7pxwUtNCadJF3vwIrszolqVEtew7NwZCeq9TiNzcgv4hZbKA1z3ypTi0xe4zAn18DeZuvr8dWht0D9Q1Ldq6+FZV01OhLrl065LKDLhp7yPXlat9zXpdeq+7mLx6rjhWoleFUW/+mBlAN2LvXUIBOwQ8MWn6jzlZ30ArktrBCrENWvWRPscxHFzVeAE+BEDQWPi+l3BniWrZOn87RTq5JW7ztPSCV9gRGgO+p7SPgn6vCj9K3As3T09vs3l7tcPDlg/TBupBAi4cvEq7XAG1XZFh9chhPxYjae4qWuQSsHTOj4SLd0ekEua4rYPfcO6SyRCPlMn2e5VgxV5X6p8Cfy2XIFlL2/A7IBALcEllElNd7rxt5g322YM7k3ko7QnIdhnBpFBpQv5tRMNI5+Yx+t3TKE3+DavySPOcs38cgr7nqUrKjWItDTmdX67yBvGhk7wb7kreWMifWEugWqMIm0VP57uOrK3Cy3bt8Lh/QsVjppgK4AbWo9C24/VJJZW+zEqte8oBsmMj1jjC2Ik5Aw4SmMqdeJt1AbSWtQ9WO7pxR8VY2J9tzT0/tu6h/9yX3mjp/LtB92e33rfQMa9DVksp/r231PthzXjy+blJhTaXxqYy8+6accembjlq5ZhZaxFKKj9Q6W+gB8tU9X78TWDuWJZU4M5oM9OY5S15LxjiqJ0GlOtOizokZlFj3jetrIYuooe4zNXXteUmL+Fy0xWPTGqhMsveqtQbeh5gro/psyfOmJLvnkiupN1zf2piNmoscu7WifDqwWrqMb86fpcrulxX265E5dV6dvT0J4CU7Ux2ySQ7sVg9bhLKV4u3yxswsM9NSvVWxMZ/Ak6rqhjlpM1lIM+lWybqLKpBVeWFEDNnaqWo991yOMT6z5O5RK83te2uCPSedI+GfPaimPlqfLR28vbdcHzQyNR3/RFUqyW71qEbH9Zrd4oFdauUvyH0LCQ73PVVHy88rNC3gvQrPbvkksnpWYC9V9Gr8sh76STVTsOtLHlIqSIGBJO8scDEXPIMjaOju6uvg0mZW6YE3vjo9zNNfWA+GWQvuWJp4HQ2Lt8aH3AbdRlBNxZ22cEpSxtBLu3atqQuW3WH47lY4udOGF2p0AaxO/Oioy4Da3Qy3vF3fvkuatpL4I8leMZbzP7LIxbLT52fUJWw4yaqA4mrpaD79Bs0Mxefdb+EOaVmZudvvpgyY/6c9el+ANxOqfLhcJ70lrxVoTKrktMVJNnycM+/tjXqckvBh9eojkX+o8xuncNPuA7uUp9Wb7WUvJbieX6ijpgG0YC3qfA2owhyyPdxZGtn/rq+nRRt6u3otn/wBL/JPbgWZ8gxNPXJa1g0emve6keFy83v1tZBr64yy5jd655tje26RDnU6X1Pykr+u7NqsuGvTOUAHbsRmN7K+MR7OWPaCiiN2lINlgE6+gGatKPdVOXbbgBdw7TVmVjv4lqAa3l8FeZQ6+289sHmmU2XA5XpdLo1Fc5fB1Hlcm7jzbUFXw9kWRiVmVM9Jtwb6h2oAl9LNgP20/XNe45xq0GEfHl57WAfuYjZf60tPf67Oq1wrw5R1YZW67QHYANuU0X5bjZ/ZtJdlPcae61ZtP5uWitWT0To4l0tlDVOujIuf7LMbIv7W6GbA/th0FSmNywTHpVy8p54vX/R5eW9Jfaq+/Xo8BT0T0h+JerM6H7Nd6bFUrm5hRL8C4DcA/IsnK/Q69C68vDoDL7Pee50fRv8WM39e78KTgh0AiOjHmPmDT1roA+kl1hl4mfXe6/x49Cze+J122unpaQf7Tju9IfQcYP/IM5T5UHqJdQZeZr33Oj8SPbnNvtNOOz0P7Wr8Tju9IbSDfaed3hB6MrAT0VcR0SeI6JNE9E1PVe65RESfT0Q/TEQ/TUQ/RUTfqOc/l4j+HhH9jB4/57nr2hIRTUT0E0T0Q/r7/UT0MW3z7yGit567jp6I6LOJ6PuI6J8Q0ceJ6Pe+kHb+s9o3/jER/a9E9Ftuva2BJwI7yUbW/yOA/xDABwD8ESL6wFOUfQHNAP4cM38AwJcB+JNa128C8FFm/iIAH9Xft0bfCODj7vdfBvCtzPw7APwagG94llqN6dsA/B1m/l0AvhhS95tuZyJ6L4A/DeCDzPy7AUwAvh6339aDWVBX/gD4vQD+rvv9zQC++SnKvkLdfwDAHwDwCQDv0XPvAfCJ565bU8/3QcDxFQB+CBLZ+y8AHHrv4Lk/AH47gJ+DOond+Vtv5/cC+AUAnwsJN/8hAP/BLbe1fZ5KjbcGMvqUnrtpIqIvAPAlAD4G4N3M/Bm99IsA3v1c9RrQXwXw55EXtsI7Afw6M8/6+9ba/P0AfgXAX1fT49uJ6LfixtuZmT8N4K8A+HkAnwHw/wD4cdx2WwPYHXRDIqLfBuBvAvgzzPwv/TUW9n0zY5ZE9AcB/DIz//hz1+UMOgD4PQD+GjN/CWTORKWy31o7A4D6EL4Gwqz+DQC/FcBXPWulNtJTgf3TAD7f/X6fnrtJIqI7CNC/i5m/X0//EhG9R6+/B8AvP1f9OvTlAP4QEf0zAN8NUeW/DcBnE5HNbLy1Nv8UgE8x88f09/dBwH/L7QwAvx/AzzHzrzDzEcD3Q9r/ltsawNOB/UcBfJF6LN+CODR+8InKPotI5hh+B4CPM/O3uEs/COBD+v1DEFv+JoiZv5mZ38fMXwBp27/PzH8MwA8D+FpNdmt1/kUAv0BEv1NPfSWAn8YNt7PSzwP4MiL6LO0rVu+bbetMT+jY+GoA/xTA/w3gv3xuZ8VKPf89iOr4fwH4Sf18NcQG/iiAnwHwfwD43Oeu66D+vw/AD+n3LwTwfwL4JIC/AeBtz12/pq7/DoAf07b+WwA+5yW0M4D/GsA/AfCPAfwvAN52623NzHu47E47vSm0O+h22ukNoR3sO+30htAO9p12ekNoB/tOO70htIN9p53eENrBvtNObwjtYN9ppzeE/n91IdPLnvRMXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0360efde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_36 (Conv2D)           (None, 94, 94, 65)        1820      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 47, 47, 65)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 45, 45, 32)        18752     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 22, 22, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 15488)             0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 100)               1548900   \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,569,775\n",
      "Trainable params: 1,569,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(65, (3,3), activation='relu', input_shape=(96,96,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "71039862",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 66.1387 - accuracy: 0.3733\n",
      "Epoch 2/25\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0353 - accuracy: 0.7267\n",
      "Epoch 3/25\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.0933 - accuracy: 0.4133\n",
      "Epoch 4/25\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.0972 - accuracy: 0.3500\n",
      "Epoch 5/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0977 - accuracy: 0.3433\n",
      "Epoch 6/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0977 - accuracy: 0.3433\n",
      "Epoch 7/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0976 - accuracy: 0.3467\n",
      "Epoch 8/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0970 - accuracy: 0.3500\n",
      "Epoch 9/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0967 - accuracy: 0.3500\n",
      "Epoch 10/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0959 - accuracy: 0.3500\n",
      "Epoch 11/25\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 1.0940 - accuracy: 0.3433\n",
      "Epoch 12/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.0795 - accuracy: 0.4333\n",
      "Epoch 13/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.8087 - accuracy: 0.7433\n",
      "Epoch 14/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.2462 - accuracy: 0.9100\n",
      "Epoch 15/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0534 - accuracy: 0.9933\n",
      "Epoch 16/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0633 - accuracy: 0.9933\n",
      "Epoch 17/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 18/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 7.5827e-04 - accuracy: 1.0000\n",
      "Epoch 19/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 7.2996e-04 - accuracy: 1.0000\n",
      "Epoch 20/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 3.3440e-04 - accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 2.0921e-04 - accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.6247e-04 - accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.3719e-04 - accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.2893e-04 - accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 1.2170e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f539067da30>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5e0e878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "학습데이터(x_train)의 이미지 개수는 300 입니다.\n",
      "x_train shape: (300, 96, 96, 3)\n",
      "y_train shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 64x64 사이즈로 바꾸어 저장합니다.\n",
    "\ttarget_size=(96,96)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "    \n",
    "\tprint(len(images), \" images resized.\")\n",
    "\t\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=96\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/test/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/test/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/test/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "x_test_reshaped = (x_test-10)/206.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_test_reshaped.shape))\n",
    "print(\"y_train shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "806c001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 1.0951 - accuracy: 0.3333\n",
      "test_loss: 1.0950698852539062 \n",
      "test_accuracy: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e58b534",
   "metadata": {},
   "source": [
    "이미지 출력 시 육안으로는 이전에 출력된 이미지에 비해 선명하다 느껴졌으나 모델에게는 더 혼란을 가져온 듯하다.\n",
    "(모델의 입장에선 해상도가 개선된 것이 아닌 크기만 커진 격이라 구분을 더 못하는 것 같다. 그리고 학습 데이터로 학습 시 accuracy가 1이 나오는 것으로 보아 그렇게 좋은 방식은 아닌 것으로 판단됨.)\n",
    "만약 해상도 부분이 같이 개선된다면 좋은 결과를 볼 수 있지 않을까 생각한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f04c2",
   "metadata": {},
   "source": [
    "### 2) 사이킷런의 RandomizedSearchCV클래스를 사용한 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9722ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300  images to be resized.\n",
      "300  images to be resized.\n",
      "300  images to be resized.\n"
     ]
    }
   ],
   "source": [
    "# (1) 필요한 모듈 import하기\n",
    "from PIL import Image \n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# (2) 데이터 준비 + Resize하기\n",
    "def resize_images(img_path):\n",
    "\timages=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "\tprint(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장\n",
    "\ttarget_size=(28,28)\n",
    "\tfor img in images:\n",
    "\t\told_img=Image.open(img)\n",
    "\t\tnew_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "\t\tnew_img.save(img, \"JPEG\")\n",
    "\n",
    "# 가위바위보 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor_1\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock_1\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper_1\"\n",
    "resize_images(image_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2aa5c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 900 입니다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=900):  # 가위바위보 이미지 개수 총합에 주의\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역 생성\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor_1/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock_1/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper_1/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c19a6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 65)        1820      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 65)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 40)        23440     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 40)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 125,663\n",
      "Trainable params: 125,663\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(65, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(40, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6123c650",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16/1261500290.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "\n",
    "params = {'filter' : randint(0, 1000),\n",
    "         'units' : randint(0, 1000)\n",
    "         }\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gs = RandomizedSearchCV(model, params, n_iter = 500, n_jobs = -1)\n",
    "\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782e7cf",
   "metadata": {},
   "source": [
    "만들어진 모델에 적합한 하이퍼파라미터를 찾아 적용해보고 싶었으나 에러가 발생했다. 발생한 원인을 찾아보아도 알 수가 없었다. 이후 딥러닝에 대해 더 공부를 한 뒤에 다시 적용해 보아야 겠다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7ebf4",
   "metadata": {},
   "source": [
    "#### 5. 자기다짐\n",
    "accuracy를 높이기 위해 여러 가지 방법을 시도해 보았으나, 만족할만한 결과물이 나오지는 못했다. 다른 방법들(대량의 학습데이터 사용, 이미지의 해상도 높이기, 정확도와 epoch와의 관계성 그래프로 나타내보기, 가중치 규제, 드롭아웃 추가, 크로스엔트로피 손실함수 등)도 적용해보고 싶었으나 아직 그만한 실력이 되지 못한다는 것만 확인한 듯하다. 현 시점에서는 위에서 정리한 모호한 점들과 내가 알게된 점을 벨로그에 정리해 확실히 가져가야 할 것이다. 그리고 적용해보지 못한 다른 방법들에 대해서는 6개월간 아이펠에서 차근차근 단계를 밟아가며 다시 시도해보고자 한다. 끝!! :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
